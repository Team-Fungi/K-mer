{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Importing Modules\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn import metrics\n","from google.colab import files\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Defining Plot\n","font_files = fm.findSystemFonts('.')\n","\n","for font_file in font_files:\n","    fm.fontManager.addfont(font_file)\n","print(font_files)\n","plt.rc('font', family='UGent Panno Text')\n","\n","plt.style.use('bmh')\n","plt.rcParams['font.size'] = 10\n","plt.rcParams['axes.labelsize'] = 10\n","plt.rcParams['axes.labelweight'] = 'bold'\n","plt.rcParams['axes.titlesize'] = 10\n","plt.rcParams['xtick.labelsize'] = 8\n","plt.rcParams['ytick.labelsize'] = 8\n","plt.rcParams['legend.fontsize'] = 10\n","plt.rcParams['figure.titlesize'] = 12\n","\n","# Model definition\n","def CNN_OHE(tax, epochsize):\n","  print('Initiating training, validation and testing on {tax} using CNN with One Hot Encoded sequences')\n","  \n","  class my_precious_dataset(Dataset):\n","    def __init__(self, x, y):\n","        self.x = torch.tensor(x, dtype=torch.float32, device='cpu')\n","        self.y = torch.tensor(y, dtype=torch.long, device='cpu') # torch.long = 부호있는 정수형, torch.float = 32비트 소수형 \n","        self.length = self.x.shape[0]\n","\n","    def __getitem__(self, idx):\n","        return self.x[idx], self.y[idx]\n","\n","    def __len__(self):\n","        return self.length\n","\n","\n","  # Load the existed Training & Validation & Testing Dataset\n","  base_path = '/content/drive/MyDrive/BachelorsProject/FinalModels/1AMBI'\n","\n","  # These files are all in my google drive\n","  TrainX = np.load(f'{base_path}Train_X_OHE1A.npy')\n","  TrainY = np.load(f'{base_path}Train_Y_{tax}1A.npy')\n","  TestX = np.load(f'{base_path}Test_X_OHE1A.npy')\n","  TestY = np.load(f'{base_path}Test_Y_{tax}1A.npy')\n","  ValX = np.load(f'{base_path}Validation_X_OHE1A.npy')\n","  ValY = np.load(f'{base_path}Validation_Y_{tax}1A.npy')\n","  print('Training, test and validation datasets are loaded...')\n","\n","  class Multi_Fungi_Classifier(nn.Module):\n","    def __init__(self):\n","        super(Multi_Fungi_Classifier, self).__init__()\n","        # First layer\n","        # Input shape = (100, 4, 700) \n","        # 100: Batch size, 4: # of nucleotide, 700: Length of the sequences \n","        self.conv1 = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)\n","        self.relu = nn.ReLU()\n","        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n","        # Input shape = (100, 8, 350)\n","        self.dropout = nn.Dropout(p=0.25)\n","        self.fc = nn.Linear(8 * 350, len(np.unique(TrainY))) \n","\n","        \n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.relu(out)\n","        out = self.pool(out)\n","        out = self.dropout(out)\n","        out = out.view(out.size(0), -1) # flatten\n","        out = self.fc(out)\n","        return out\n","  \n","    print('Model constructed...')\n","\n","  model = Multi_Fungi_Classifier()\n","\n","  batches = 200\n","\n","  # Dataset\n","  trainset = my_precious_dataset(TrainX, TrainY)\n","  valset = my_precious_dataset(ValX, ValY)\n","  testset = my_precious_dataset(TestX, TestY)\n","\n","  # Dataloader\n","  trainloader = DataLoader(trainset, batch_size=batches, shuffle=True)\n","  valloader = DataLoader(valset, batch_size=batches, shuffle=False)\n","  testloader = DataLoader(testset, batch_size=batches, shuffle=False)  \n","\n","  # Model Training\n","  loss_fn = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","  # Store the training loss and the accuracy\n","  training_losses = []\n","  training_accuracy = []\n","\n","  # Store the validation loss and the accuracy\n","  validation_losses = []\n","  validation_accuracy = []\n","      \n","  print('Training model...')\n","  epochs = epochsize\n","  for epoch in range(epochs):\n","      model.train()\n","      training_loss = 0.0\n","      correct = 0 # 예측값이 맞은 횟수\n","      total = 0\n","\n","      # x_train is input data for the batch, y_train is the labels\n","      for i, (x_train, y_train) in enumerate(trainloader):\n","          input = x_train.transpose(1, 2)\n","          output = model(input)                        # Output shape: torch.Size([100, 16])\n","          loss = loss_fn(output, y_train)\n","\n","          # Add L2 regularization\n","          l2_reg = torch.tensor(0.)\n","          for param in model.parameters():\n","              l2_reg += torch.norm(param)\n","          loss += 0.001 * l2_reg\n","\n","          # Back propagation - gradients are calculated and the optimizer updates\n","          optimizer.zero_grad()\n","          loss.backward()\n","          optimizer.step()\n","\n","          # Calculate training loss and accuracy\n","          training_loss += loss.item() * input.size(0) # multiply the loss with the batch size (100)\n","          _, predicted = torch.max(output.data, 1)     # 0은 행 (세로), 1은 열 (가로) 마다 최댓값의 위치를 예측값으로 사용하겠다는 의미! * 원래 torch.max 는 최댓값, 최댓값의 위치를 산출해주는데, 최댓값은 필요없으므로 _ 로 저장 x\n","          total += y_train.size(0)                     # Total number of predictions \n","          correct += (predicted == y_train).sum().item() # 예측값과 라벨이 맞을때의 개수 * item()이 없으면 tensor(64) 라고 나옴. \n","\n","      # Print training statistics\n","      epoch_loss = training_loss / len(trainloader.dataset) # Training loss is calculated by dividing the cumulative loss by the total number of data points in the training dataset\n","      epoch_acc = 100. * correct / total   \n","      \n","      # Store the training loss and the accuracy\n","      training_losses.append(epoch_loss)\n","      training_accuracy.append(epoch_acc)\n","      \n","      # Validation\n","      model.eval()\n","      validation_loss = 0.0\n","      correct = 0\n","      total = 0\n","    \n","      with torch.no_grad():\n","        for i, (x_val, y_val) in enumerate(valloader):\n","            input = x_val.transpose(1, 2)\n","            output = model(input)\n","            loss = loss_fn(output, y_val)\n","            \n","            # Calculate validation loss and accuracy\n","            validation_loss += loss.item() * input.size(0)\n","            _, predicted = torch.max(output.data, 1)\n","            total += y_val.size(0)\n","            correct += (predicted == y_val).sum().item()\n","\n","      # Calculate validation loss and accuracy\n","      epoch_val_loss = validation_loss / len(valloader.dataset)\n","      epoch_val_acc = 100. * correct / total\n","\n","      # Store the validation loss and the accuracy\n","      validation_losses.append(epoch_val_loss)\n","      validation_accuracy.append(epoch_val_acc)\n","\n","      print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {epoch_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}, Training Accuracy: {epoch_acc:.2f}%, Validation Accuracy: {epoch_val_acc:.2f}%')\n","\n","  # Set the model to evaluation mode\n","  model.eval()\n","\n","  with torch.no_grad():\n","    y_prediction = []\n","    y_true = []\n","\n","    for x_test, y_test in testloader:\n","      test_input = x_test.transpose(1, 2)\n","      test_output = model(test_input)\n","      y_prediction += torch.argmax(test_output, dim=1).tolist()\n","      y_true += y_test.tolist()\n","    print(metrics.classification_report(y_true, y_prediction, digits=3))     \n","\n","  plt.plot(training_losses, label='Training', color='#1E64C8', linewidth=1)\n","  plt.plot(validation_losses, label='Validation', color='black', linewidth=1)\n","  plt.title(f'Training and Validation Loss of the CNN on {tax} level with OHE')\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Loss (in %)')\n","  plt.legend()\n","  plt.savefig(f'CNNOHE{tax}{kmer}Loss1A.svg')\n","  files.download(f'CNNOHE{tax}{kmer}Loss1A.svg') \n","  plt.show()\n","\n","  plt.plot(training_accuracies, label='Training', color='#1E64C8', linewidth=1)\n","  plt.plot(validation_accuracies, label='Validation', color='black', linewidth=1)\n","  plt.title(f'Training and Validation Accuracy of the CNN on {tax} level with {kmer}')\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Accuracy (in %)')\n","  plt.legend()\n","  plt.savefig(f'CNNOHE{tax}Accuracy1A.svg')\n","  files.download(f'CNNOHE{tax}Accuracy1A.svg') \n","  plt.show()\n","  print(f'Training, validation and testing on {tax} level with OHE is completed.') "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We then run the model on all taxonomic levels with 3mer, 4mer and 5mer."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Running the Model on 3mer, 4mer, 5mer\n","# 3 mer\n","CNNK('3mer', 'phylum', 100)\n","CNNK('3mer', 'class', 100)\n","CNNK('3mer', 'order', 100)\n","CNNK('3mer', 'family', 100)\n","CNNK('3mer', 'genus', 100)\n","\n","# 4 mer\n","CNNK('4mer', 'phylum', 100)\n","CNNK('4mer', 'class', 100)\n","CNNK('4mer', 'order', 100)\n","CNNK('4mer', 'family', 100)\n","CNNK('4mer', 'genus', 100)\n","\n","# 5 mer\n","CNNK('5mer', 'phylum', 100)\n","CNNK('5mer', 'class', 100)\n","CNNK('5mer', 'order', 100)\n","CNNK('5mer', 'family', 100)\n","CNNK('5mer', 'genus', 100)"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
